{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMw9530UrZt_"
      },
      "outputs": [],
      "source": [
        "pip install \"ag2[gemini]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import ConversableAgent"
      ],
      "metadata": {
        "id": "gIpkHujLrfJD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "api_key=userdata.get('Gemini_main')\n",
        "os.environ[\"GEMINI_API_KEY\"] = api_key"
      ],
      "metadata": {
        "id": "AoJ3tF_jtpp_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_config = {\n",
        "  \"config_list\": [\n",
        "    {\n",
        "      \"api_type\": \"gemini\",\n",
        "      \"model\": \"gemini-2.0-flash-lite\",\n",
        "      \"api_key\": api_key,\n",
        "      \"api_type\": \"google\"\n",
        "    }\n",
        "  ],\n",
        "}"
      ],
      "metadata": {
        "id": "SSbs787LtsLs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "personal_information_agent = ConversableAgent(\n",
        "    name=\"Onboarding Personal Information Agent\",\n",
        "    system_message='''You are a helpful customer onboarding agent who works for TBH.AI,\n",
        "    you are here to help new customers get started with our AI consultation service.\n",
        "    Your job is to gather customer's name and location.\n",
        "    Do not ask for other information. Return 'TERMINATE'\n",
        "    when you have gathered all the information.''',\n",
        "    llm_config=llm_config,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ],
      "metadata": {
        "id": "5ke-DcHku2DU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onboarding_topic_preference_agent = ConversableAgent(\n",
        "    name=\"Onboarding Topic preference Agent\",\n",
        "    system_message='''You are a helpful customer onboarding agent who works for TBH.AI,\n",
        "    you are here to help new customers by consulting the user doubts about AI and AI project requirements\n",
        "    Your job is to gather customer's AI and AI project management requirements.\n",
        "    Do not ask for other information.\n",
        "    Return 'TERMINATE' when you have gathered all the information.''',\n",
        "    llm_config=llm_config,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ],
      "metadata": {
        "id": "J3C10r-avW3r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_engagement_agent = ConversableAgent(\n",
        "    name=\"Customer Engagement Agent\",\n",
        "    system_message='''You are a helpful and knowledgeable customer engagement agent at TBH.AI.\n",
        "Your role is to assist users with AI and AI project management questions, providing insightful recommendations, best practices, and guidance.\n",
        "Ensure that the interaction remains engaging and informative, tailoring responses to the user's specific needs.\n",
        "Do not ask for unrelated information.\n",
        "Return 'TERMINATE' when you have provided sufficient assistance.''',\n",
        "    llm_config=llm_config,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ],
      "metadata": {
        "id": "bDx_qJ8xwA5Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_proxy_agent = ConversableAgent(\n",
        "    name=\"customer_proxy_agent\",\n",
        "    llm_config=False,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"ALWAYS\",\n",
        "    is_termination_msg=lambda msg: \"terminate\" in msg.get(\"content\").lower(),\n",
        ")"
      ],
      "metadata": {
        "id": "WT4wPwy3w-9b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chats = [\n",
        "    {\n",
        "        \"sender\": personal_information_agent,\n",
        "        \"recipient\": customer_proxy_agent,\n",
        "        \"message\":\n",
        "            \"Hello, I'm here to help you get started with our product.\"\n",
        "            \"Could you tell me your name and location and what organisation the user work for?\",\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "        \"summary_args\": {\n",
        "            \"summary_prompt\" : \"Return the customer information \"\n",
        "                             \"into as JSON object only: \"\n",
        "                             \"{'name': '', 'location': '','Organisation': ''}\",  ## Custom summary_prompt\n",
        "        },\n",
        "        \"max_turns\": 1,\n",
        "        \"clear_history\" : True\n",
        "    },\n",
        "\n",
        "  {\n",
        "    \"sender\": customer_engagement_agent,\n",
        "    \"recipient\": customer_proxy_agent,\n",
        "    \"message\":\n",
        "        \"Great! How can I assist you with your AI and AI project management questions?\",\n",
        "    \"summary_method\": \"reflection_with_llm\",\n",
        "    \"max_turns\": 1,\n",
        "    \"clear_history\": False\n",
        "  },\n",
        "\n",
        "    {\n",
        "        \"sender\": customer_proxy_agent,\n",
        "        \"recipient\": customer_engagement_agent,\n",
        "        \"message\": \"Let's find something fun to read.\",\n",
        "        \"max_turns\": 1,\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "d7Lvv6ZJxCRf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import initiate_chats\n",
        "\n",
        "chat_results = initiate_chats(chats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykY5bB1MxvRx",
        "outputId": "5ca9380a-870e-4412-e053-79789d8f273b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Onboarding Personal Information Agent (to customer_proxy_agent):\n",
            "\n",
            "Hello, I'm here to help you get started with our product.Could you tell me your name and location and what organisation the user work for?\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/autogen/agentchat/chat.py:57: UserWarning: Repetitive recipients detected: The chat history will be cleared by default if a recipient appears more than once. To retain the chat history, please set 'clear_history=False' in the configuration of the repeating agent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replying as customer_proxy_agent. Provide feedback to Onboarding Personal Information Agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: Hello i am saish from India and work for ABC.inc\n",
            "customer_proxy_agent (to Onboarding Personal Information Agent):\n",
            "\n",
            "Hello i am saish from India and work for ABC.inc\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Customer Engagement Agent (to customer_proxy_agent):\n",
            "\n",
            "Great! How can I assist you with your AI and AI project management questions?\n",
            "Context: \n",
            "```json\n",
            "{\n",
            "  \"name\": \"saish\",\n",
            "  \"location\": \"India\",\n",
            "  \"Organisation\": \"ABC.inc\"\n",
            "}\n",
            "```\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as customer_proxy_agent. Provide feedback to Customer Engagement Agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: well i am confused between what to use ,RAG or Finetuning.I have data as pdf and i want to make a custom chatbot\n",
            "customer_proxy_agent (to Customer Engagement Agent):\n",
            "\n",
            "well i am confused between what to use ,RAG or Finetuning.I have data as pdf and i want to make a custom chatbot\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "customer_proxy_agent (to Customer Engagement Agent):\n",
            "\n",
            "Let's find something fun to read.\n",
            "Context: \n",
            "```json\n",
            "{\n",
            "  \"name\": \"saish\",\n",
            "  \"location\": \"India\",\n",
            "  \"Organisation\": \"ABC.inc\"\n",
            "}\n",
            "```\n",
            "\n",
            "The user is seeking guidance on whether to use RAG or fine-tuning for building a custom chatbot using PDF data.\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Customer Engagement Agent (to customer_proxy_agent):\n",
            "\n",
            "Okay, Saish from ABC.inc in India! That's a great question, and a common one when building chatbots with PDF data. Let's break down RAG and fine-tuning to help you decide the best path.\n",
            "\n",
            "**RAG (Retrieval-Augmented Generation):**\n",
            "\n",
            "*   **How it works:** RAG combines a large language model (LLM) with a retrieval system. Your PDF data is first processed (chunked, embedded, and indexed). When a user asks a question, the retrieval system finds relevant information from your PDFs, and then the LLM uses this information to generate a response.\n",
            "*   **Pros:**\n",
            "    *   **Up-to-date Information:** Easily incorporates new PDF data without retraining the model.\n",
            "    *   **Transparency:** You can often trace a response back to its source document, which can be useful for verifying accuracy.\n",
            "    *   **Cost-Effective:** Generally less expensive than fine-tuning, especially for frequent data updates.\n",
            "    *   **Avoids Overfitting:** The model doesn't memorize your data, reducing the risk of providing incorrect information on unrelated queries.\n",
            "*   **Cons:**\n",
            "    *   **Performance Depends on Retrieval:** The quality of the retrieval system is critical. If it doesn't find the right information, the LLM can't generate a good answer.\n",
            "    *   **Complexity:** Setting up a RAG system can be more complex, involving data processing, indexing, and integrating with an LLM.\n",
            "    *   **May Require More Prompt Engineering:** To ensure the LLM uses the retrieved information effectively, you might need to experiment with prompt design.\n",
            "\n",
            "**Fine-tuning:**\n",
            "\n",
            "*   **How it works:** You take a pre-trained LLM and \"fine-tune\" it by training it on your specific PDF data. The model adjusts its internal parameters to better understand and generate responses based on your data.\n",
            "*   **Pros:**\n",
            "    *   **Potential for High Accuracy:** If done well, fine-tuning can result in a chatbot that's very accurate and tailored to your specific content.\n",
            "    *   **More Control:** You have more control over the model's behavior and the style of its responses.\n",
            "    *   **Can handle complex relationships:** Fine-tuning can enable the model to learn complex relationships between concepts in your data.\n",
            "*   **Cons:**\n",
            "    *   **Data Intensive:** Requires a large, high-quality dataset of questions and answers based on your PDFs.\n",
            "    *   **Expensive:** Training a model can be computationally expensive, especially for large datasets.\n",
            "    *   **Requires Retraining:** Any updates to your PDF data usually require retraining the model, which can be time-consuming and costly.\n",
            "    *   **Risk of Overfitting:** The model may memorize the training data, leading to poor performance on unseen or slightly different queries.\n",
            "\n",
            "**Recommendation:**\n",
            "\n",
            "Based on your situation, I would recommend starting with **RAG**.\n",
            "\n",
            "*   **Initial Setup:** RAG is usually faster to set up and deploy. You can start by chunking your PDFs, creating embeddings, and testing different retrieval strategies.\n",
            "*   **Data Updates:** If your PDF data changes frequently, RAG is much easier to update.\n",
            "*   **Cost:** RAG is generally more cost-effective, especially in the long run.\n",
            "*   **Iterative Approach**: You can always switch to fine-tuning if RAG isn't delivering the desired results.\n",
            "\n",
            "**Best Practices:**\n",
            "\n",
            "*   **Experiment with Chunking Strategies:** How you divide your PDFs into chunks will significantly affect retrieval performance. Experiment with different chunk sizes and overlap.\n",
            "*   **Choose the Right Embedding Model:** Select an embedding model that works well with your data type and the LLM you plan to use.\n",
            "*   **Prompt Engineering is Key:** Carefully design your prompts to instruct the LLM on how to use the retrieved information.\n",
            "*   **Evaluate and Iterate:** Continuously evaluate the chatbot's performance and make adjustments to your retrieval system, prompts, and even the LLM used.\n",
            "\n",
            "I hope this helps you get started. Do you have any specific questions about RAG or fine-tuning that I can address?\n",
            "TERMINATE\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chat_result in chat_results:\n",
        "    print(chat_result.summary)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrgjAZLUxx-S",
        "outputId": "0269e562-86c8-4abf-e17b-976b0a7e2cee"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'content': '```json\\n{\\n  \"name\": \"saish\",\\n  \"location\": \"India\",\\n  \"Organisation\": \"ABC.inc\"\\n}\\n```\\n', 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None}\n",
            "\n",
            "\n",
            "{'content': 'The user is seeking guidance on whether to use RAG or fine-tuning for building a custom chatbot using PDF data.\\n', 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None}\n",
            "\n",
            "\n",
            "{'content': 'For building a custom chatbot with PDF data, RAG is generally recommended over fine-tuning due to its flexibility, cost-effectiveness, and ease of updating with new information, particularly for someone like Saish at ABC.inc. Start with RAG and iterate.\\n', 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chat_result in chat_results:\n",
        "    print(chat_result.cost)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6gvF1juzMrx",
        "outputId": "63b541fe-4f4d-4e01-f013-9f2528b43756"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'usage_including_cached_inference': {'total_cost': 8.1075e-05, 'gemini-2.0-flash-lite': {'cost': 8.1075e-05, 'prompt_tokens': 737, 'completion_tokens': 86, 'total_tokens': 823}}, 'usage_excluding_cached_inference': {'total_cost': 8.1075e-05, 'gemini-2.0-flash-lite': {'cost': 8.1075e-05, 'prompt_tokens': 737, 'completion_tokens': 86, 'total_tokens': 823}}}\n",
            "\n",
            "\n",
            "{'usage_including_cached_inference': {'total_cost': 1.47e-05, 'gemini-2.0-flash-lite': {'cost': 1.47e-05, 'prompt_tokens': 100, 'completion_tokens': 24, 'total_tokens': 124}}, 'usage_excluding_cached_inference': {'total_cost': 1.47e-05, 'gemini-2.0-flash-lite': {'cost': 1.47e-05, 'prompt_tokens': 100, 'completion_tokens': 24, 'total_tokens': 124}}}\n",
            "\n",
            "\n",
            "{'usage_including_cached_inference': {'total_cost': 0.00037634999999999996, 'gemini-2.0-flash-lite': {'cost': 0.00037634999999999996, 'prompt_tokens': 1218, 'completion_tokens': 950, 'total_tokens': 2168}}, 'usage_excluding_cached_inference': {'total_cost': 0.00037634999999999996, 'gemini-2.0-flash-lite': {'cost': 0.00037634999999999996, 'prompt_tokens': 1218, 'completion_tokens': 950, 'total_tokens': 2168}}}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LlJjRxS8zRmB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}